# UHOK ML Inference Service

ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ML ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
**â€»ì—°ë™ë˜ëŠ” ë°±ì—”ë“œ ë²„ì „: v3.0.0**

## ğŸ¯ ëª©ì 

- **ë¹„ìš© ìµœì í™”**: ë¬´ê±°ìš´ ML ëª¨ë¸ ë° ë²¡í„° ê²€ìƒ‰ ë¡œì§ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš© ì ˆì•½
- **í™•ì¥ì„±**: ML ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª¨ë¸ ë° ê²€ìƒ‰ ë¡œì§ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”
- **ì—­í•  ë¶„ë¦¬**: ML ê´€ë ¨ ë¡œì§(ì„ë² ë”©, ê²€ìƒ‰)ì„ ML ì„œë¹„ìŠ¤ê°€ ì „ë‹´

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ

- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬
- **SentenceTransformers**: ë¬¸ì¥ ì„ë² ë”© ìƒì„± (paraphrase-multilingual-MiniLM-L12-v2)
- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (CPU ì „ìš©)
- **SQLAlchemy**: ë¹„ë™ê¸° ORM ë° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°/ì¿¼ë¦¬
- **psycopg**: PostgreSQL ë¹„ë™ê¸° ë“œë¼ì´ë²„
- **pgvector**: PostgreSQL ë²¡í„° í™•ì¥ ë° SQLAlchemy ì§€ì›
- **Docker**: ì»¨í…Œì´ë„ˆí™”ëœ ë°°í¬

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[Backend Service] --HTTP--> [ML Inference Service] --SQL--> [PostgreSQL]
                                    â†“
                             [SentenceTransformer]
                             [paraphrase-multilingual-MiniLM-L12-v2]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ë¡œì»¬ ê°œë°œ
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# .env íŒŒì¼ ìƒì„± ë° DB ì—°ê²° ì •ë³´ ì„¤ì • (ì˜ˆ: POSTGRES_RECOMMEND_URL="postgresql+psycopg_async://user:password@localhost:5432/REC_DB")
# ê°œë°œ ì„œë²„ ì‹¤í–‰
python -m app.main
```

### Dockerë¡œ ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (DB ì—°ê²° ì •ë³´ í•„ìš”)
docker run -p 8001:8001 --env-file .env uhok-ml-inference
```

### Docker Composeë¡œ ì‹¤í–‰ (ê¶Œì¥)
```bash
# uhok-deployì˜ ml í´ë”ì—ì„œ ì‹¤í–‰
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml up -d
```

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)
- **ì²˜ë¦¬ëŸ‰**: CPU ê¸°ë°˜, ë‹¨ì¼ ì›Œì»¤
- **ì§€ì—°ì‹œê°„**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨
- **ë©”ëª¨ë¦¬**: ì•½ 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)

## ğŸ—ºï¸ API ì—”ë“œí¬ì¸íŠ¸

- `POST /api/v1/embed`: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
- `POST /api/v1/embed-batch`: ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
- `POST /api/v1/search`: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìœ ì‚¬ ë ˆì‹œí”¼ ê²€ìƒ‰ (ì„ë² ë”© ìƒì„± ë° DB ê²€ìƒ‰ í¬í•¨)
- `GET /api/v1/model-info`: í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜
- `GET /health`: ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬

## ğŸ”„ ë°±ì—”ë“œ ì—°ë™

ë°±ì—”ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì›ê²© ML ì„œë¹„ìŠ¤ì˜ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=30.0  # ëª¨ë¸ ë¡œë”© ë° DB ê²€ìƒ‰ ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
ML_RETRIES=2

# ì›ê²© ìœ ì‚¬ë„ ê²€ìƒ‰ í˜¸ì¶œ
async with httpx.AsyncClient(timeout=ML_TIMEOUT) as client:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/search",
        json={
            "query": "ë§¤ì½¤í•œ ë‹­ë³¶ìŒíƒ•",
            "top_k": 10,
            "exclude_ids": [123, 456] # ì„ íƒ ì‚¬í•­
        }
    )
    response.raise_for_status()
    result = response.json()
    search_results = result["results"] # [{'recipe_id': 789, 'distance': 0.123}, ...]
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/search",
        json={
            "query": "ë§¤ì½¤í•œ ë‹­ë³¶ìŒíƒ•",
            "top_k": 10
        }
    )
    response.raise_for_status()
    return response.json()["results"]
except httpx.TimeoutException:
    logger.error("ML ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ")
    return []
except httpx.HTTPStatusError as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ HTTP ì—ëŸ¬: {e.response.status_code} - {e.response.text}")
    return []
except Exception as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    return []
```
